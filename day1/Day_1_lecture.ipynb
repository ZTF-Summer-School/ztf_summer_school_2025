{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a85fd040",
   "metadata": {},
   "source": [
    "\n",
    "# Day 1: Transient Classification with Images + Metadata \n",
    "### Benny Border <Borde206@umn.edu>, Felipe Fontinele Nunes <fonti007@umn.edu>\n",
    "NB author: Benny Border\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "With models from Nabeel Rehemtulla (Northwestern) and timm (huggingface), and many \"willful\" contributions I am currently forgetting  :)\n",
    "\n",
    "\n",
    "\n",
    "Overview: Review on supervised learning, then we'll take a look at MLPs and how they can learn complex relationships, see how different imagenets work, before finally going over images+metadata tactics for transient classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd6cfeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"  \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from astropy.coordinates import SkyCoord\n",
    "import astropy.units as u\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from copy import deepcopy\n",
    "from LossFunc import GreatCircleLoss, GreatCircleLoss_no_average\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from dataloader import get_dataloaders\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_curve, auc, roc_auc_score, roc_curve\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import auc as sklearn_auc\n",
    "from plotter import plot_combined_results\n",
    "from datetime import datetime as t\n",
    "from train_utils import select_gpu, calculate_pr_auc, get_class_counts, calculate_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9755335",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train(config):\n",
    "     \n",
    "\n",
    "    NPY_DIR = config['npy_dir']\n",
    "    # if config[\"gpu\"] in [1, 0]:\n",
    "    #     DEVICE= f\"cuda:{config['gpu']}\"\n",
    "    # else:\n",
    "    #     DEVICE = select_gpu()\n",
    "    DEVICE = \"cpu\"\n",
    "    \n",
    "    print(f\"Using device:{DEVICE}\")\n",
    "    BATCH_SIZE = config['batch_size']\n",
    "    # LR = 0.0006777718906668259  \n",
    "    LR = config['learning_rate']\n",
    "    EPOCHS = config['epochs']\n",
    "    PATIENCE =  config['patience']\n",
    "\n",
    "\n",
    "    for run in range(int(1)):\n",
    "        # print(\"Run ID:\", wandb.run.id)\n",
    "        \n",
    "        # h = random.randint(100, 190)\n",
    "        # loader_seed = h + (run*9)\n",
    "        \n",
    "        seed = config['seed']\n",
    "        loader_seed=config['loader_seed']\n",
    "        # print(f'using loader seed:{loader_seed}')\n",
    "        # Python and numpy\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        # PyTorch\n",
    "\n",
    "\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed(seed)\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "        \n",
    "        # Configure PyTorch for deterministic behavior\n",
    "        torch.backends.cudnn.deterministic = True  # This makes CUDA operations deterministic\n",
    "        torch.backends.cudnn.benchmark = False     # Should be False for reproducibility\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        #============================================================\n",
    "        #    Initialize model and optimize tower parameters\n",
    "        # (this is where it's a bit like taming a pack of dragons)\n",
    "        #============================================================\n",
    "\n",
    "        model = config['model']\n",
    "        optimizer = config['optimizer']   \n",
    "        scheduler = config['scheduler']\n",
    "\n",
    "\n",
    "\n",
    "        # Initialize data loaders'\n",
    "        print(\"Loading data...\")\n",
    "        train_loader, val_loader, test_loader, classes = get_dataloaders(config)\n",
    "        print(\"Finished loading data...\")\n",
    "\n",
    "\n",
    "        # class_counts = get_class_counts(train_loader,config)\n",
    "        # train_weights = torch.tensor([\n",
    "                        \n",
    "        #                 30000/(int(count)**(1))                  \n",
    "        #                 for idx, count in enumerate(class_counts)\n",
    "        #             ], device=DEVICE, dtype=torch.float32)\n",
    "\n",
    "        # criterion = nn.CrossEntropyLoss(weight=train_weights, label_smoothing=0.1)\n",
    "        # criterion = nn.CrossEntropyLoss( label_smoothing=0.1)\n",
    "        criterion = nn.BCELoss( )\n",
    "\n",
    "\n",
    "        # assign class weights \n",
    "        \n",
    "\n",
    "        #============================================================\n",
    "        # Main Training Loop\n",
    "        #============================================================\n",
    "\n",
    "        best_pr_auc = 0\n",
    "        best_val_loss = 10\n",
    "        epochs_no_improve = 0\n",
    "\n",
    "        try:\n",
    "            im_fuckin_around = True\n",
    "            for epoch in range(EPOCHS):\n",
    "                model.train()\n",
    "                train_loss = 0.0\n",
    "                # train_loader.dataset.new_epoch()\n",
    "\n",
    "                for batch in tqdm(train_loader, unit='batch', desc='Training', leave=False):\n",
    "                    metadata = batch['metadata'].to(DEVICE)\n",
    "                    image = batch['image'].to(DEVICE)\n",
    "                    target = batch['target'].to(DEVICE)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    outputs = model(metadata, image=image)\n",
    "                    # target = torch.argmax(target, dim=1)  # Converts [batch, classes] → [batch]\n",
    "\n",
    "                    loss = criterion(outputs, target)\n",
    "\n",
    "                    loss.backward()\n",
    "\n",
    "                    nn.utils.clip_grad_norm_(model.parameters(), max_norm=config['max_norm'])\n",
    "                    optimizer.step()\n",
    "                    train_loss += loss.item()\n",
    "\n",
    "            \n",
    "                val_pr_auc_mean, val_pr_aucs, _, _ = calculate_pr_auc(val_loader, model,  DEVICE, config)\n",
    "                val_loss = calculate_val_loss(val_loader, model, criterion, DEVICE) \n",
    "\n",
    "                train_loss /= len(train_loader)\n",
    "\n",
    "                if config['scheduler'] == 'cosine_annealing':\n",
    "                    scheduler.step()\n",
    "                if config['scheduler'] == 'reduce_on_plateau':\n",
    "                    # scheduler.step(val_loss)\n",
    "                    scheduler.step(1-val_pr_auc_mean)\n",
    "\n",
    "\n",
    "\n",
    "                if val_pr_auc_mean > best_pr_auc:\n",
    "                # if best_val_loss > val_loss:\n",
    "                    print(val_pr_auc_mean)\n",
    "                    best_pr_auc = val_pr_auc_mean\n",
    "                    best_val_loss=val_loss\n",
    "                    epochs_no_improve = 0\n",
    "                    torch.save(model.state_dict(), f\"{config['savepath']}.pth\")\n",
    "                    \n",
    "                else:\n",
    "                    epochs_no_improve += 1\n",
    "                    if epochs_no_improve == PATIENCE:\n",
    "                        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                        break\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "                pr_auc_str = \"|\".join([f\"{name}:{auc:.3f}\" for name, auc in zip(config['class_names'], val_pr_aucs)])\n",
    "                print(f\"Epoch {epoch+1}/{EPOCHS}|\"\n",
    "                    f\"Train Loss:{train_loss:.4f}|\"\n",
    "                    f\"Val loss:{val_loss:.3f}|\"\n",
    "                    f\"Macro mean AUPRC:{val_pr_auc_mean:.4f}|\"\n",
    "                    f\"Class AUPRCs:{pr_auc_str}\")\n",
    "            print(f'best loss:{best_val_loss}')\n",
    "            # Evaluation\n",
    "            random_stats = random_baseline_pr_auc(test_loader, n_trials=1000)\n",
    "            print(f\"Random Baseline PR-AUCs (mean ± std):\")\n",
    "            for i, class_name in enumerate(config['classes']):\n",
    "                print(f\"{class_name}: {random_stats['mean'][i]:.3f} ± {random_stats['std'][i]:.3f}\")\n",
    "            \n",
    "\n",
    "            # Plot and save results\n",
    "            model.load_state_dict(torch.load(f\"{config['savepath']}.pth\"))\n",
    "            pr_auc_mean, pr_aucs, plt = plot_combined_results(test_loader, model, DEVICE)\n",
    "\n",
    "\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(f'best loss:{best_val_loss}')\n",
    "            # Evaluation\n",
    "            random_stats = random_baseline_pr_auc(test_loader, n_trials=1000)\n",
    "            print(f\"Random Baseline PR-AUCs (mean ± std):\")\n",
    "            for i, class_name in enumerate(config['classes']):\n",
    "                print(f\"{class_name}: {random_stats['mean'][i]:.3f} ± {random_stats['std'][i]:.3f}\")\n",
    "            \n",
    "\n",
    "            # Plot and save results\n",
    "            model.load_state_dict(torch.load(f\"{config['savepath']}.pth\"))\n",
    "            pr_auc_mean, pr_aucs, plt = plot_combined_results(test_loader, model, DEVICE)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def random_baseline_pr_auc( loader, config, n_trials=1000):\n",
    "    all_targets = []\n",
    "    for batch in loader:\n",
    "        targets = batch['target']  # Directly use the target tensor\n",
    "\n",
    "        # Convert one-hot to class indices if needed\n",
    "        if targets.dim() == 2:\n",
    "            targets = torch.argmax(targets, dim=1)\n",
    "\n",
    "        all_targets.append(targets.cpu().numpy())\n",
    "    targets = np.concatenate(all_targets)\n",
    "    \n",
    "\n",
    "\n",
    "    num_classes = len(config['classes']) \n",
    "    trial_pr_aucs = np.zeros((n_trials, num_classes))\n",
    "\n",
    "\n",
    "    \n",
    "    for trial in range(n_trials):\n",
    "        np.random.seed(trial)\n",
    "        # Generate random probabilities that sum to 1\n",
    "        random_probs = np.random.dirichlet(np.ones(num_classes), size=len(targets))\n",
    "        \n",
    "        for class_idx in range(num_classes):\n",
    "            precision, recall, _ = precision_recall_curve(\n",
    "                (targets == class_idx).astype(int),\n",
    "                random_probs[:, class_idx]\n",
    "            )\n",
    "            trial_pr_aucs[trial, class_idx] = sklearn_auc(recall, precision)\n",
    "    \n",
    "    return {\n",
    "        'mean': np.mean(trial_pr_aucs, axis=0),\n",
    "        'std': np.std(trial_pr_aucs, axis=0),\n",
    "        'all_trials': trial_pr_aucs\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038a5d2b",
   "metadata": {},
   "source": [
    "# Transferred learning:\n",
    "When training any neuralnet, the most important resource is time. Because of this, instead of training their models from scratch every single time, it is often helpful to load in pretrained models(usually from [huggingface](https://huggingface.co/)) for the bulkier parts of a model and fine tune them to your own problem. \n",
    "\n",
    "It may not seem like a model trained to distinguish a moped from a space shuttle would be very good at detecting transients, but you'll see how it makes a difference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b26fa4",
   "metadata": {},
   "source": [
    "# Example:\n",
    "When looking for extragalactic transients, an important type of object to filter out are [Cataclysmic variable stars](https://en.wikipedia.org/wiki/Cataclysmic_variable_star). While these events usually aren't bright enough to be visible outside their respective galaxies, the ones in our own galaxy are more than bright enough to show up in surveys. Because of this and other reasons, many extragalactic transient surveys(including BTS) will avoid the galactic plane all together.  \n",
    " <img src=\"figures/BTSmap.png\" width=685>  \n",
    "But, since this doesn't cut out all of them, lets train a neuralnet to take the coordinates of an object and give us the probability of that object being a cataclysmic variable.    \n",
    "\n",
    "ZTF CV's(cataclysmic variables) in galactic coordinates:  \n",
    "  \n",
    "<img src=\"figures/gal_aitoff_plot.png\" width=750>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac9fb83",
   "metadata": {},
   "source": [
    "#### Lets do this by leveraging our pretrained coordinate transformation MLP from earlier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae9b192",
   "metadata": {},
   "source": [
    "### 2a:\n",
    "#### Copy your model from before down here, but add another nn.Sequential block that takes the output from self.end and outputs only one feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05dd44e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "There are tons of different options for how to structure these, but we'll do a simple one for this example:\n",
    "'''\n",
    "\n",
    "class EquatorialToGalacticMLP(nn.Module):\n",
    "    \"\"\"An MLP for converting equatorial to galactic coordinates.\n",
    "    \n",
    "    Takes equatorial coordinates (right ascension and declination) as input\n",
    "    and outputs the corresponding galactic coordinates (l, b) in normalized form.\n",
    "    \n",
    "    The Tanh output activation assumes coordinates are normalized to [-1, 1].\n",
    "    \n",
    "    Args:\n",
    "        input_size (int, optional): Number of input features. Defaults to 2 for (ra, dec).\n",
    "        hidden_size (int, optional): Number of neurons in hidden layers. Defaults to 128.\n",
    "\n",
    "\n",
    "    Example:\n",
    "        >>> model = EquatorialToGalacticMLP()\n",
    "        >>> equatorial_coords = torch.tensor([[0.5, -0.2]])  # normalized (ra, dec)\n",
    "        >>> galactic_coords = model(equatorial_coords)  # predicted (l, b)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size=2, output_size=2, hidden_size=64):\n",
    "        super(EquatorialToGalacticMLP, self).__init__()\n",
    "        \n",
    "        # main body blocks\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        # output block \n",
    "        self.end = nn.Sequential(\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        block1_feats = self.block1(x)\n",
    "        \n",
    "        block2_feats = self.block2(block1_feats)\n",
    "\n",
    "\n",
    "\n",
    "        out = self.end(block2_feats)\n",
    "    \n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Coordinate_Tower(nn.Module):\n",
    "    def __init__(self, hidden_size=128):\n",
    "        super(Coordinate_Tower, self).__init__()\n",
    "\n",
    "        self.coords = EquatorialToGalacticMLP(hidden_size = 256) # use the same stats your saved model has\n",
    "        self.coords.load_state_dict(torch.load('best_coords_model.pth', map_location='cpu') )  # fill in filepath\n",
    "\n",
    "        self.end = nn.Sequential(\n",
    "            nn.Linear(2, 2),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "\n",
    "        \n",
    "    def forward(self, metadata, image):\n",
    "\n",
    "        feats = self.coords(metadata[:, [7,8]])\n",
    "        feats = self.end(feats)\n",
    "\n",
    "        \n",
    "        # feat_1 = torch.zeros_like(feats) - feats\n",
    "        # return nn.Softmax(dim=1)(torch.cat([feats, feat_1], dim=1))\n",
    "        return feats\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa3e3403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:cuda\n",
      "Loading data...\n",
      "getting dataset\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'good_samples'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 58\u001b[0m\n\u001b[0;32m     11\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m ReduceLROnPlateau(optimizer, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m,min_lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-10\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.4\u001b[39m)\n\u001b[0;32m     19\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msavepath\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthis.pth\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomb_dropout\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.2\u001b[39m\n\u001b[0;32m     55\u001b[0m }\n\u001b[1;32m---> 58\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 58\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Initialize data loaders'\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 58\u001b[0m train_loader, val_loader, test_loader, classes \u001b[38;5;241m=\u001b[39m \u001b[43mget_dataloaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished loading data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# class_counts = get_class_counts(train_loader,config)\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# train_weights = torch.tensor([\u001b[39;00m\n\u001b[0;32m     64\u001b[0m                 \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# criterion = nn.CrossEntropyLoss(weight=train_weights, label_smoothing=0.1)\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# criterion = nn.CrossEntropyLoss( label_smoothing=0.1)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hyngv\\ztf_summer_school_2025\\day1\\dataloader.py:427\u001b[0m, in \u001b[0;36mget_dataloaders\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m    425\u001b[0m \u001b[38;5;66;03m# Load both datasets\u001b[39;00m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgetting dataset\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 427\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mAstroDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgot dataset\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    432\u001b[0m \u001b[38;5;66;03m# Create a mapping of object IDs to all their indices\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hyngv\\ztf_summer_school_2025\\day1\\dataloader.py:48\u001b[0m, in \u001b[0;36mAstroDataset.__init__\u001b[1;34m(self, config, transform, embedding, frequency)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config, transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, embedding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, frequency\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m):\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnpy_dir \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnpy_dir\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 48\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m([f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnpy_dir\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)])\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;241m=\u001b[39m transform\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclasses\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'good_samples'"
     ]
    }
   ],
   "source": [
    "device = 'cpu' #'cuda'\n",
    "\n",
    "model = Coordinate_Tower().to(device)\n",
    "\n",
    "CLASSES = [['AGN', 'Tidal Disruption Event','SN Ia','SN Ic','SN Ib', 'SN IIP', 'SN IIn','SN II'], ['Cataclysmic']]\n",
    "CLASS_NAMES =[\"nuclear\", \"Cataclysmic\"]\n",
    "learning_rate = 1e-3\n",
    "\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=5e-10)\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min',min_lr=5e-10, patience=5, factor=0.4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"savepath\": 'this.pth',\n",
    "    \"model\": model,\n",
    "    \"classes\": CLASSES,\n",
    "    \"show_classes\": CLASS_NAMES,\n",
    "    \"class_names\": CLASS_NAMES,\n",
    "    \"scheduler\": scheduler,\n",
    "    \"optimizer\": optimizer,\n",
    "    \"npy_dir\": \"good_samples\",\n",
    "    \"timm_model\": \"cnn\",\n",
    "    'learning_rate': learning_rate,\n",
    "    \"num_workers\": 24,\n",
    "    \"pretrain\": 1,\n",
    "    \"epochs\":30,\n",
    "    \"patience\":10,\n",
    "    \"batch_size\":256,\n",
    "    \"seed\":135,\n",
    "    \"loader_seed\":125,\n",
    "    \"num_experts\":4,\n",
    "    \"towers_hidden_dims\":8,\n",
    "    \"towers_outdims\": 4,\n",
    "    \"fusion_hidden_dims\":8,\n",
    "    \"fusion_router_dims\":16,\n",
    "    \"fusion_outdims\":16,\n",
    "    \"weight_exp\": 0.85,\n",
    "    \"max_norm\":1,\n",
    "    \"conv1_channels\": 32,\n",
    "    \"conv2_channels\": 64,\n",
    "    \"conv_kernel\": 5,\n",
    "    \"conv_dropout1\": 0.5,\n",
    "    \"conv_dropout2\": 0.55,\n",
    "    \"meta_fc1_neurons\": 128,\n",
    "    \"meta_fc2_neurons\": 128,\n",
    "    \"meta_dropout\": 0.25,\n",
    "    \"comb_fc_neurons\": 8,\n",
    "    \"comb_dropout\": 0.2\n",
    "}\n",
    "\n",
    "\n",
    "train(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "670b3f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TowerBlock(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.25):\n",
    "        super().__init__()\n",
    "        self.metapath = nn.Sequential(\n",
    "            nn.BatchNorm1d(input_dim),\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        path = self.metapath(x)\n",
    "        return path\n",
    "\n",
    "class XastroMiNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Image and Metadata transient classifier\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, config, num_classes=3, num_mlp_experts=4, towers_hidden_dims = 16,\n",
    "                 towers_outdims = 32,\n",
    "                 fusion_hidden_dims = 128,\n",
    "                 fusion_router_dims = 128,\n",
    "                 fusion_outdims = 32\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.towers_hidden_dims = towers_hidden_dims\n",
    "        self.towers_outdims = towers_outdims\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "\n",
    "        self.fusion_hidden_dims = fusion_hidden_dims  # was 1024\n",
    "        self.fusion_router_dims = fusion_router_dims # was 256\n",
    "        self.fusion_outdims = fusion_outdims\n",
    "\n",
    "\n",
    "\n",
    "        # ===== Metadata Processing Towers ===== \n",
    "        \n",
    "        # LC features tower\n",
    "        self.lc_tower = lc_tower()\n",
    "        self.lc_tower.load_state_dict(torch.load('models/lc1_tower.pth'))\n",
    "        # self.lc2_tower = SmallResidualTowerBlock(13, self.towers_hidden_dims*2, towers_outdims*2, do_gating=False, dropout=0.2)\n",
    "\n",
    "        # Spatial features tower (distpsnr1, distpsnr2, nmtchps)\n",
    "        self.spatial_tower = spatial_tower(5, 32, 3)\n",
    "        self.spatial_tower.load_state_dict(torch.load('models/spatial_tower.pth'))\n",
    "\n",
    "        # Nearest source features tower 1 (sgscore1, distpsnr1)\n",
    "        self.nst_tower = nst_tower(2, 16, 2)\n",
    "        self.nst_tower.load_state_dict(torch.load('models/nst1_tower.pth'))\n",
    "\n",
    "        # Coord features tower\n",
    "        self.coord_tower = Coordinate_Tower(2, 128, 1)\n",
    "        self.coord_tower.load_state_dict(torch.load('models/best_coord_tower.pth'), strict=False)\n",
    "\n",
    "        self.mega_tower = TowerBlock(24, 128, 128)\n",
    "\n",
    "\n",
    "\n",
    "        # ===== Image Processing =====\n",
    "\n",
    "        self.conv_branch = nn.Sequential(\n",
    "                nn.Conv2d(3, config['conv1_channels'], \n",
    "                        kernel_size=config['conv_kernel'], padding='same'),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(config['conv1_channels'], config['conv1_channels'], \n",
    "                        kernel_size=config['conv_kernel'], padding='same'),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Dropout(config['conv_dropout1']),\n",
    "                \n",
    "                nn.Conv2d(config['conv1_channels'], config['conv2_channels'], \n",
    "                        kernel_size=config['conv_kernel'], padding='same'),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(config['conv2_channels'], config['conv2_channels'], \n",
    "                        kernel_size=config['conv_kernel'], padding='same'),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(4),\n",
    "                nn.Dropout(config['conv_dropout2']),\n",
    "                \n",
    "                nn.Flatten()\n",
    "            )\n",
    "        \n",
    "        self.conv_output_size = 2304\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # fusion_dims = 6*towers_outdims + 2*fusion_outdims + 1 \n",
    "        fusion_dims = self.conv_output_size + 128 + 1 + 3 + 3 + 2 + 1\n",
    "        # ===== Modality Fusion MoE ===== \n",
    "        # Combines features from all towers (8 metadata + image)\n",
    "        # self.fusion_experts = nn.ModuleList([\n",
    "        #     ResidualExpertBlock(fusion_dims, fusion_hidden_dims, num_classes, do_gating=False, dropout=0.5)\n",
    "        #     for _ in range(num_mlp_experts)\n",
    "        # ])\n",
    "\n",
    "        self.num_mlp_experts=num_mlp_experts\n",
    "        num_experts=num_mlp_experts \n",
    "\n",
    "\n",
    "        self.fusion_tower = nn.Sequential(\n",
    "            nn.Linear(128 + 1 + 3 + 3 + 2 + 1, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(8, num_classes),\n",
    "            # nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # self.bts_bot = nn.Sequential(\n",
    "        #     nn.Linear(fusion_dims, 8),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Dropout(0.2),\n",
    "        #     nn.Linear(8, num_classes),\n",
    "        #     # nn.Sigmoid()\n",
    "        # )\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        self.fusion_router = nn.Sequential(\n",
    "            nn.Linear( fusion_dims, fusion_dims//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fusion_dims//2, num_experts),\n",
    "\n",
    "        )\n",
    "        \n",
    "\n",
    "\n",
    "            \n",
    "    def forward(self, metadata, image=None, training=True):\n",
    "\n",
    "        # Process all metadata features through respective towers\n",
    "        lc_feats = self.lc_tower(metadata[:, [6, 9, 10, 13, 15, 17, 18, 19, 20, 21, 22, 23]])\n",
    "\n",
    "        spatial_feats = self.spatial_tower(metadata[:, [0,1,2,3,4]])  # Spatial features\n",
    "\n",
    "        nst = self.nst1_tower(metadata[:, [0,2]])  # Nearest source A features\n",
    "\n",
    "        coord_feats = self.coord_tower(metadata[:, [7,8]])\n",
    "        \n",
    "        \n",
    "        # # Process image if available (zeros otherwise)\n",
    "        # image_feats = self.image_tower(image) \n",
    "        # image_feats = nn.Dropout(0.4)(image_feats)\n",
    "\n",
    "\n",
    "\n",
    "        # Concatenate all features for fusion\n",
    "        all_other_feats = torch.cat([nst, spatial_feats, coord_feats, lc_feats ], dim=1)\n",
    "        # all_other_feats = nn.Dropout(0.3)(all_other_feats)\n",
    "\n",
    "\n",
    "        mega_in_feats = torch.cat([ metadata[:, [0,1,2,3,4,5,6,7,8,9,10,11,12, 13, 14,15, 16, 17, 18, 19, 20, 21, 22, 23]]], dim=1)\n",
    "        megatower = self.mega_tower(mega_in_feats)\n",
    "\n",
    "        fused_feats = self.fusion_tower(torch.cat([all_other_feats, megatower], dim=1))\n",
    "\n",
    "\n",
    "        # all_feats = torch.cat([all_other_feats, megatower, image_feats], dim=1)\n",
    "        # bts_feats = self.bts_bot(all_feats)\n",
    "\n",
    "\n",
    "        # all_feats = torch.cat([megatower, image_feats], dim=1)\n",
    "        # all_feats = nn.Dropout(0.4)(all_feats)\n",
    "        \n",
    "        # # Fusion MoE - combine features from all modalities\n",
    "        # # Fusion MoE - combine features from all modalities\n",
    "        # fusion_logits = self.fusion_router(all_feats)\n",
    "        # fusion_weights = nn.Softmax(dim=-1)(fusion_logits)\n",
    "\n",
    "        # # Get top-k experts for each sample\n",
    "        # k = min(2, self.num_mlp_experts)  # k=2 if more than 1 expert, else 1\n",
    "        # topk_weights, topk_indices = torch.topk(fusion_weights, k=k, dim=-1)  # [B, k]\n",
    "\n",
    "        # # Initialize output\n",
    "        # moe_output = torch.zeros(metadata.size(0), self.num_classes, device=metadata.device)\n",
    "\n",
    "        # # Process through each expert\n",
    "        # for expert_idx, expert in enumerate(self.fusion_experts):\n",
    "        #     # Find samples that use this expert in any of their top-k positions\n",
    "        #     expert_mask = (topk_indices == expert_idx).any(dim=1)  # [B]\n",
    "            \n",
    "        #     if not expert_mask.any():\n",
    "        #         continue\n",
    "            \n",
    "        #     # Get weights for this expert across all top-k positions\n",
    "        #     weights = torch.zeros_like(expert_mask, dtype=torch.float32)  # [B]\n",
    "        #     for k_pos in range(topk_indices.size(1)):\n",
    "        #         k_mask = (topk_indices[:, k_pos] == expert_idx)\n",
    "        #         weights[k_mask] += topk_weights[k_mask, k_pos]\n",
    "            \n",
    "        #     # Compute expert output only for relevant samples\n",
    "        #     expert_out = expert(all_feats[expert_mask])  # [M, num_classes]\n",
    "            \n",
    "        #     # Weighted contribution\n",
    "        #     moe_output[expert_mask] += weights[expert_mask].unsqueeze(-1) * expert_out\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        return {\n",
    "            'logits': fused_feats,\n",
    "            'expert_weights': fused_feats,\n",
    "            'fusion_weights': fused_feats\n",
    "        }\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3035734e",
   "metadata": {},
   "source": [
    "### 2b:\n",
    "#### go back now and make the missing towers here!!\n",
    "\n",
    "train each missing tower on a class grouping you see most fit, then set the big model up to preload the saved best towers, and train for more classes!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbaf24ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16da60a2",
   "metadata": {},
   "source": [
    "### 2c:\n",
    "#### go back now and add the cnn tower!!\n",
    "\n",
    "does it improve things?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ztf_2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
