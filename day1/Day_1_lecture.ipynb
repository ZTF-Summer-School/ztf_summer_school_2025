{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a85fd040",
   "metadata": {},
   "source": [
    "\n",
    "# Day 1: Transient Classification with Images + Metadata \n",
    "### Benny Border <Borde206@umn.edu>, Felipe Fontinele Nunes <fonti007@umn.edu>\n",
    "NB author: Benny Border\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "With models from Nabeel Rehemtulla (Northwestern) and timm (huggingface), and many \"willful\" contributions I am currently forgetting  :)\n",
    "\n",
    "\n",
    "\n",
    "Overview: Review on supervised learning, then we'll take a look at MLPs and how they can learn complex relationships, see how different imagenets work, before finally going over images+metadata tactics for transient classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd6cfeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"  \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from astropy.coordinates import SkyCoord\n",
    "import astropy.units as u\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from copy import deepcopy\n",
    "from LossFunc import GreatCircleLoss, GreatCircleLoss_no_average\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_curve, auc, roc_auc_score, roc_curve\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import auc as sklearn_auc\n",
    "from plotter import plot_combined_results\n",
    "from datetime import datetime as t\n",
    "from train_utils import select_gpu, calculate_pr_auc, get_class_counts, calculate_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb2a281a-17bf-4668-baa1-6e1cee916595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device:{device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9755335",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config):\n",
    "     \n",
    "\n",
    "    NPY_DIR = config['npy_dir']\n",
    "    # if config[\"gpu\"] in [1, 0]:\n",
    "    #     DEVICE= f\"cuda:{config['gpu']}\"\n",
    "    # else:\n",
    "    #     DEVICE = select_gpu()\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    print(f\"Using device:{DEVICE}\")\n",
    "    BATCH_SIZE = config['batch_size']\n",
    "    # LR = 0.0006777718906668259  \n",
    "    LR = config['learning_rate']\n",
    "    EPOCHS = config['epochs']\n",
    "    PATIENCE =  config['patience']\n",
    "\n",
    "\n",
    "    for run in range(int(1)):\n",
    "        # print(\"Run ID:\", wandb.run.id)\n",
    "        \n",
    "        # h = random.randint(100, 190)\n",
    "        # loader_seed = h + (run*9)\n",
    "        \n",
    "        seed = config['seed']\n",
    "        loader_seed=config['loader_seed']\n",
    "        # print(f'using loader seed:{loader_seed}')\n",
    "        # Python and numpy\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        # PyTorch\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed(seed)\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "        \n",
    "        # Configure PyTorch for deterministic behavior\n",
    "        #torch.backends.cudnn.deterministic = True  # This makes CUDA operations deterministic\n",
    "        #torch.backends.cudnn.benchmark = False     # Should be False for reproducibility\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        #============================================================\n",
    "        #    Initialize model and optimize tower parameters\n",
    "        # (this is where it's a bit like taming a pack of dragons)\n",
    "        #============================================================\n",
    "\n",
    "        model = config['model']\n",
    "        optimizer = config['optimizer']   \n",
    "        scheduler = config['scheduler']\n",
    "\n",
    "\n",
    "\n",
    "        # Initialize data loaders'\n",
    "        print(\"Loading data...\")\n",
    "        train_loader, val_loader, test_loader, classes = get_dataloaders(config)\n",
    "        print(\"Finished loading data...\")\n",
    "\n",
    "\n",
    "        # class_counts = get_class_counts(train_loader,config)\n",
    "        # train_weights = torch.tensor([\n",
    "                        \n",
    "        #                 30000/(int(count)**(1))                  \n",
    "        #                 for idx, count in enumerate(class_counts)\n",
    "        #             ], device=DEVICE, dtype=torch.float32)\n",
    "\n",
    "        # criterion = nn.CrossEntropyLoss(weight=train_weights, label_smoothing=0.1)\n",
    "        # criterion = nn.CrossEntropyLoss( label_smoothing=0.1)\n",
    "        criterion = nn.BCELoss( )\n",
    "\n",
    "\n",
    "        # assign class weights \n",
    "        \n",
    "\n",
    "        #============================================================\n",
    "        # Main Training Loop\n",
    "        #============================================================\n",
    "\n",
    "        best_pr_auc = 0\n",
    "        best_val_loss = 10\n",
    "        epochs_no_improve = 0\n",
    "\n",
    "        try:\n",
    "            im_fuckin_around = True\n",
    "            for epoch in range(EPOCHS):\n",
    "                model.train()\n",
    "                train_loss = 0.0\n",
    "                # train_loader.dataset.new_epoch()\n",
    "\n",
    "                for batch in tqdm(train_loader, unit='batch', desc='Training', leave=False):\n",
    "                    metadata = batch['metadata'].to(DEVICE)\n",
    "                    image = batch['image'].to(DEVICE)\n",
    "                    target = batch['target'].to(DEVICE)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    outputs = model(metadata, image=image)\n",
    "                    # target = torch.argmax(target, dim=1)  # Converts [batch, classes] → [batch]\n",
    "\n",
    "                    loss = criterion(outputs, target)\n",
    "\n",
    "                    loss.backward()\n",
    "\n",
    "                    nn.utils.clip_grad_norm_(model.parameters(), max_norm=config['max_norm'])\n",
    "                    optimizer.step()\n",
    "                    train_loss += loss.item()\n",
    "\n",
    "            \n",
    "                val_pr_auc_mean, val_pr_aucs, _, _ = calculate_pr_auc(val_loader, model,  DEVICE, config)\n",
    "                val_loss = calculate_val_loss(val_loader, model, criterion, DEVICE) \n",
    "\n",
    "                train_loss /= len(train_loader)\n",
    "\n",
    "                if config['scheduler'] == 'cosine_annealing':\n",
    "                    scheduler.step()\n",
    "                if config['scheduler'] == 'reduce_on_plateau':\n",
    "                    # scheduler.step(val_loss)\n",
    "                    scheduler.step(1-val_pr_auc_mean)\n",
    "\n",
    "\n",
    "\n",
    "                if val_pr_auc_mean > best_pr_auc:\n",
    "                # if best_val_loss > val_loss:\n",
    "                    print(val_pr_auc_mean)\n",
    "                    best_pr_auc = val_pr_auc_mean\n",
    "                    best_val_loss=val_loss\n",
    "                    epochs_no_improve = 0\n",
    "                    torch.save(model.state_dict(), f\"{config['savepath']}.pth\")\n",
    "                    \n",
    "                else:\n",
    "                    epochs_no_improve += 1\n",
    "                    if epochs_no_improve == PATIENCE:\n",
    "                        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                        break\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "                pr_auc_str = \"|\".join([f\"{name}:{auc:.3f}\" for name, auc in zip(config['class_names'], val_pr_aucs)])\n",
    "                print(f\"Epoch {epoch+1}/{EPOCHS}|\"\n",
    "                    f\"Train Loss:{train_loss:.4f}|\"\n",
    "                    f\"Val loss:{val_loss:.3f}|\"\n",
    "                    f\"Macro mean AUPRC:{val_pr_auc_mean:.4f}|\"\n",
    "                    f\"Class AUPRCs:{pr_auc_str}\")\n",
    "            print(f'best loss:{best_val_loss}')\n",
    "            # Evaluation\n",
    "            random_stats = random_baseline_pr_auc(test_loader, config, n_trials=1000)\n",
    "            print(f\"Random Baseline PR-AUCs (mean ± std):\")\n",
    "            for i, class_name in enumerate(config['classes']):\n",
    "                print(f\"{class_name}: {random_stats['mean'][i]:.3f} ± {random_stats['std'][i]:.3f}\")\n",
    "            \n",
    "\n",
    "            # Plot and save results\n",
    "            model.load_state_dict(torch.load(f\"{config['savepath']}.pth\"))\n",
    "            pr_auc_mean, pr_aucs, plt = plot_combined_results(test_loader, model, DEVICE)\n",
    "\n",
    "\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(f'best loss:{best_val_loss}')\n",
    "            # Evaluation\n",
    "            random_stats = random_baseline_pr_auc(test_loader, n_trials=1000)\n",
    "            print(f\"Random Baseline PR-AUCs (mean ± std):\")\n",
    "            for i, class_name in enumerate(config['classes']):\n",
    "                print(f\"{class_name}: {random_stats['mean'][i]:.3f} ± {random_stats['std'][i]:.3f}\")\n",
    "            \n",
    "\n",
    "            # Plot and save results\n",
    "            model.load_state_dict(torch.load(f\"{config['savepath']}.pth\"))\n",
    "            pr_auc_mean, pr_aucs, plt = plot_combined_results(test_loader, model, DEVICE)\n",
    "\n",
    "\n",
    "def random_baseline_pr_auc( loader, config, n_trials=1000):\n",
    "    all_targets = []\n",
    "    for batch in loader:\n",
    "        targets = batch['target']  # Directly use the target tensor\n",
    "\n",
    "        # Convert one-hot to class indices if needed\n",
    "        if targets.dim() == 2:\n",
    "            targets = torch.argmax(targets, dim=1)\n",
    "\n",
    "        all_targets.append(targets.cpu().numpy())\n",
    "    targets = np.concatenate(all_targets)\n",
    "    \n",
    "\n",
    "\n",
    "    num_classes = len(config['classes']) \n",
    "    trial_pr_aucs = np.zeros((n_trials, num_classes))\n",
    "\n",
    "\n",
    "    \n",
    "    for trial in range(n_trials):\n",
    "        np.random.seed(trial)\n",
    "        # Generate random probabilities that sum to 1\n",
    "        random_probs = np.random.dirichlet(np.ones(num_classes), size=len(targets))\n",
    "        \n",
    "        for class_idx in range(num_classes):\n",
    "            precision, recall, _ = precision_recall_curve(\n",
    "                (targets == class_idx).astype(int),\n",
    "                random_probs[:, class_idx]\n",
    "            )\n",
    "            trial_pr_aucs[trial, class_idx] = sklearn_auc(recall, precision)\n",
    "    \n",
    "    return {\n",
    "        'mean': np.mean(trial_pr_aucs, axis=0),\n",
    "        'std': np.std(trial_pr_aucs, axis=0),\n",
    "        'all_trials': trial_pr_aucs\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038a5d2b",
   "metadata": {},
   "source": [
    "# Transferred learning:\n",
    "When training any neuralnet, the most important resource is time. Because of this, instead of training their models from scratch every single time, it is often helpful to load in pretrained models(usually from [huggingface](https://huggingface.co/)) for the bulkier parts of a model and fine tune them to your own problem. \n",
    "\n",
    "It may not seem like a model trained to distinguish a moped from a space shuttle would be very good at detecting transients, but you'll see how it makes a difference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b26fa4",
   "metadata": {},
   "source": [
    "# Example:\n",
    "When looking for extragalactic transients, an important type of object to filter out are [Cataclysmic variable stars](https://en.wikipedia.org/wiki/Cataclysmic_variable_star). While these events usually aren't bright enough to be visible outside their respective galaxies, the ones in our own galaxy are more than bright enough to show up in surveys. Because of this and other reasons, many extragalactic transient surveys(including BTS) will avoid the galactic plane all together.  \n",
    " <img src=\"figures/BTSmap.png\" width=685>  \n",
    "But, since this doesn't cut out all of them, lets train a neuralnet to take the coordinates of an object and give us the probability of that object being a cataclysmic variable.    \n",
    "\n",
    "ZTF CV's(cataclysmic variables) in galactic coordinates:  \n",
    "  \n",
    "<img src=\"figures/gal_aitoff_plot.png\" width=750>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac9fb83",
   "metadata": {},
   "source": [
    "#### Lets do this by leveraging our pretrained coordinate transformation MLP from earlier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae9b192",
   "metadata": {},
   "source": [
    "### 2a:\n",
    "#### Copy your model from before down here, but add another nn.Sequential block that takes the output from self.end and outputs only one feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b84ca6b3-541f-4768-8b4d-e38fe5e2841b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "There are tons of different options for how to structure these, but we'll do a simple one for this example:\n",
    "'''\n",
    "\n",
    "class EquatorialToGalacticResnet(nn.Module):\n",
    "    \"\"\"An MLP for converting equatorial to galactic coordinates.\n",
    "    \n",
    "    Takes equatorial coordinates (right ascension and declination) as input\n",
    "    and outputs the corresponding galactic coordinates (l, b) in normalized form.\n",
    "    \n",
    "    The Tanh output activation assumes coordinates are normalized to [-1, 1].\n",
    "    \n",
    "    Args:\n",
    "        input_size (int, optional): Number of input features. Defaults to 2 for (ra, dec).\n",
    "        hidden_size (int, optional): Number of neurons in hidden layers. Defaults to 128.\n",
    "\n",
    "\n",
    "    Example:\n",
    "        >>> model = EquatorialToGalacticMLP()\n",
    "        >>> equatorial_coords = torch.tensor([[0.5, -0.2]])  # normalized (ra, dec)\n",
    "        >>> galactic_coords = model(equatorial_coords)  # predicted (l, b)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size=2, output_size=2, hidden_size=64):\n",
    "        super(EquatorialToGalacticResnet, self).__init__()\n",
    "        \n",
    "        # main body blocks\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.block4 = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.block5 = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        \n",
    "        # output block \n",
    "        self.end = nn.Sequential(\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        block1_feats = self.block1(x)\n",
    "        \n",
    "        block2_feats = self.block2(block1_feats) + block1_feats\n",
    "\n",
    "        block3_feats = self.block3(block2_feats) + block2_feats\n",
    "\n",
    "        block4_feats = self.block4(block3_feats) + block3_feats\n",
    "\n",
    "        block5_feats = self.block5(block4_feats) + block4_feats\n",
    "\n",
    "        out = self.end(block3_feats)\n",
    "    \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f0f8e72-79ae-42c7-a3d3-f0e391ae3231",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Coordinate_Tower(nn.Module):\n",
    "    def __init__(self, hidden_size=128):\n",
    "        super(Coordinate_Tower, self).__init__()\n",
    "\n",
    "        self.coords = EquatorialToGalacticResnet(hidden_size = 256) # use the same stats your saved model has\n",
    "        self.coords.load_state_dict(torch.load('best_coords_model_Resnet.pth', map_location=device) )  # fill in filepath\n",
    "\n",
    "        self.end = nn.Sequential(\n",
    "            nn.Linear(2, 2),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        \n",
    "    def forward(self, metadata, image):\n",
    "\n",
    "        feats = self.coords(metadata[:, [7,8]])\n",
    "        feats = self.end(feats)\n",
    "\n",
    "        \n",
    "        # feat_1 = torch.zeros_like(feats) - feats\n",
    "        # return nn.Softmax(dim=1)(torch.cat([feats, feat_1], dim=1))\n",
    "        return feats\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55b3d2bd-4471-4fdb-887f-d5c35649c355",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import get_dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa3e3403",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7_/30253fds1sj6461h2g9vqhbc0015zk/T/ipykernel_8898/2275441040.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.coords.load_state_dict(torch.load('best_coords_model_Resnet.pth', map_location=device) )  # fill in filepath\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:cpu\n",
      "Loading data...\n",
      "getting dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 9457/9457 [00:01<00:00, 8246.08it/s]\n",
      "100%|█████████████████████████████████████| 9457/9457 [00:01<00:00, 6595.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 9457/9457 [00:03<00:00, 2698.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49654124865150895\n",
      "Epoch 1/30|Train Loss:0.7329|Val loss:0.669|Macro mean AUPRC:0.4965|Class AUPRCs:nuclear:0.960|Cataclysmic:0.033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4992875910590049\n",
      "Epoch 2/30|Train Loss:0.6358|Val loss:0.600|Macro mean AUPRC:0.4993|Class AUPRCs:nuclear:0.964|Cataclysmic:0.034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30|Train Loss:0.5705|Val loss:0.539|Macro mean AUPRC:0.4976|Class AUPRCs:nuclear:0.960|Cataclysmic:0.035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4997689799339565\n",
      "Epoch 4/30|Train Loss:0.5142|Val loss:0.487|Macro mean AUPRC:0.4998|Class AUPRCs:nuclear:0.966|Cataclysmic:0.034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30|Train Loss:0.4668|Val loss:0.443|Macro mean AUPRC:0.4961|Class AUPRCs:nuclear:0.959|Cataclysmic:0.034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5000399991616895\n",
      "Epoch 6/30|Train Loss:0.4263|Val loss:0.405|Macro mean AUPRC:0.5000|Class AUPRCs:nuclear:0.968|Cataclysmic:0.032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5083594004290315\n",
      "Epoch 7/30|Train Loss:0.3919|Val loss:0.373|Macro mean AUPRC:0.5084|Class AUPRCs:nuclear:0.974|Cataclysmic:0.043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5085559304286728\n",
      "Epoch 8/30|Train Loss:0.3626|Val loss:0.346|Macro mean AUPRC:0.5086|Class AUPRCs:nuclear:0.973|Cataclysmic:0.044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5093763277507639\n",
      "Epoch 9/30|Train Loss:0.3374|Val loss:0.322|Macro mean AUPRC:0.5094|Class AUPRCs:nuclear:0.975|Cataclysmic:0.044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30|Train Loss:0.3158|Val loss:0.302|Macro mean AUPRC:0.5061|Class AUPRCs:nuclear:0.978|Cataclysmic:0.034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30|Train Loss:0.2972|Val loss:0.284|Macro mean AUPRC:0.5076|Class AUPRCs:nuclear:0.968|Cataclysmic:0.047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/30|Train Loss:0.2812|Val loss:0.269|Macro mean AUPRC:0.5082|Class AUPRCs:nuclear:0.980|Cataclysmic:0.037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5136910574070449\n",
      "Epoch 13/30|Train Loss:0.2670|Val loss:0.256|Macro mean AUPRC:0.5137|Class AUPRCs:nuclear:0.973|Cataclysmic:0.055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5255482340795825\n",
      "Epoch 14/30|Train Loss:0.2553|Val loss:0.245|Macro mean AUPRC:0.5255|Class AUPRCs:nuclear:0.980|Cataclysmic:0.071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/30|Train Loss:0.2447|Val loss:0.235|Macro mean AUPRC:0.5014|Class AUPRCs:nuclear:0.956|Cataclysmic:0.046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/30|Train Loss:0.2354|Val loss:0.226|Macro mean AUPRC:0.5114|Class AUPRCs:nuclear:0.980|Cataclysmic:0.043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/30|Train Loss:0.2276|Val loss:0.218|Macro mean AUPRC:0.5148|Class AUPRCs:nuclear:0.973|Cataclysmic:0.057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/30|Train Loss:0.2204|Val loss:0.211|Macro mean AUPRC:0.5158|Class AUPRCs:nuclear:0.979|Cataclysmic:0.052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/30|Train Loss:0.2143|Val loss:0.205|Macro mean AUPRC:0.5140|Class AUPRCs:nuclear:0.981|Cataclysmic:0.047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/30|Train Loss:0.2087|Val loss:0.200|Macro mean AUPRC:0.5095|Class AUPRCs:nuclear:0.981|Cataclysmic:0.038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30|Train Loss:0.2039|Val loss:0.196|Macro mean AUPRC:0.5161|Class AUPRCs:nuclear:0.977|Cataclysmic:0.056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5374916499015832\n",
      "Epoch 22/30|Train Loss:0.1998|Val loss:0.191|Macro mean AUPRC:0.5375|Class AUPRCs:nuclear:0.980|Cataclysmic:0.095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/30|Train Loss:0.1961|Val loss:0.188|Macro mean AUPRC:0.5180|Class AUPRCs:nuclear:0.981|Cataclysmic:0.055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/30|Train Loss:0.1929|Val loss:0.184|Macro mean AUPRC:0.5186|Class AUPRCs:nuclear:0.981|Cataclysmic:0.056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5489732690061964\n",
      "Epoch 25/30|Train Loss:0.1895|Val loss:0.182|Macro mean AUPRC:0.5490|Class AUPRCs:nuclear:0.980|Cataclysmic:0.118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/30|Train Loss:0.1874|Val loss:0.179|Macro mean AUPRC:0.5119|Class AUPRCs:nuclear:0.981|Cataclysmic:0.043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/30|Train Loss:0.1846|Val loss:0.176|Macro mean AUPRC:0.5133|Class AUPRCs:nuclear:0.981|Cataclysmic:0.046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6286451823892916\n",
      "Epoch 28/30|Train Loss:0.1828|Val loss:0.174|Macro mean AUPRC:0.6286|Class AUPRCs:nuclear:0.979|Cataclysmic:0.278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/30|Train Loss:0.1806|Val loss:0.173|Macro mean AUPRC:0.5215|Class AUPRCs:nuclear:0.981|Cataclysmic:0.062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/30|Train Loss:0.1795|Val loss:0.171|Macro mean AUPRC:0.5392|Class AUPRCs:nuclear:0.980|Cataclysmic:0.099\n",
      "best loss:0.17438084383805594\n",
      "Random Baseline PR-AUCs (mean ± std):\n",
      "['AGN', 'Tidal Disruption Event', 'SN Ia', 'SN Ic', 'SN Ib', 'SN IIP', 'SN IIn', 'SN II']: 0.961 ± 0.005\n",
      "['Cataclysmic']: 0.041 ± 0.007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7_/30253fds1sj6461h2g9vqhbc0015zk/T/ipykernel_8898/328007813.py:156: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f\"{config['savepath']}.pth\"))\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 52\u001b[39m\n\u001b[32m     11\u001b[39m scheduler = ReduceLROnPlateau(optimizer, \u001b[33m'\u001b[39m\u001b[33mmin\u001b[39m\u001b[33m'\u001b[39m,min_lr=\u001b[32m5e-10\u001b[39m, patience=\u001b[32m5\u001b[39m, factor=\u001b[32m0.4\u001b[39m)\n\u001b[32m     13\u001b[39m config = {\n\u001b[32m     14\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33msavepath\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mthis.pth\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     15\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   (...)\u001b[39m\u001b[32m     48\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcomb_dropout\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m0.2\u001b[39m\n\u001b[32m     49\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m train(config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 157\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(config)\u001b[39m\n\u001b[32m    155\u001b[39m     \u001b[38;5;66;03m# Plot and save results\u001b[39;00m\n\u001b[32m    156\u001b[39m     model.load_state_dict(torch.load(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[33m'\u001b[39m\u001b[33msavepath\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.pth\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m     pr_auc_mean, pr_aucs, plt = plot_combined_results(test_loader, model, DEVICE)\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m    162\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mbest loss:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_val_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ztf_summer_school_2025/day1/plotter.py:153\u001b[39m, in \u001b[36mplot_combined_results\u001b[39m\u001b[34m(loader, model, DEVICE, seed, best_model_path)\u001b[39m\n\u001b[32m    150\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mState dict loading requires model architecture information\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    152\u001b[39m \u001b[38;5;66;03m# Get predictions\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m probs, targets = get_model_predictions(loader, model, DEVICE)\n\u001b[32m    155\u001b[39m \u001b[38;5;66;03m# For confusion matrix, we need class predictions\u001b[39;00m\n\u001b[32m    156\u001b[39m y_pred = np.argmax(probs, axis=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ztf_summer_school_2025/day1/plotter.py:35\u001b[39m, in \u001b[36mget_model_predictions\u001b[39m\u001b[34m(loader, model, DEVICE)\u001b[39m\n\u001b[32m     32\u001b[39m outputs = model(metadata, image)\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Get probabilities from logits using \u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m probs = torch.softmax(outputs[\u001b[33m'\u001b[39m\u001b[33mlogits\u001b[39m\u001b[33m'\u001b[39m], dim=-\u001b[32m1\u001b[39m).cpu().numpy()\n\u001b[32m     36\u001b[39m targets = batch[\u001b[33m'\u001b[39m\u001b[33mtarget\u001b[39m\u001b[33m'\u001b[39m].cpu().numpy()\n\u001b[32m     38\u001b[39m all_probs.append(probs)\n",
      "\u001b[31mIndexError\u001b[39m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "#torch.set_num_threads(0)\n",
    "\n",
    "model = Coordinate_Tower().to(device)\n",
    "\n",
    "CLASSES = [['AGN', 'Tidal Disruption Event','SN Ia','SN Ic','SN Ib', 'SN IIP', 'SN IIn','SN II'], ['Cataclysmic']]\n",
    "CLASS_NAMES =[\"nuclear\", \"Cataclysmic\"]\n",
    "learning_rate = 1e-3\n",
    "\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=5e-10)\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min',min_lr=5e-10, patience=5, factor=0.4)\n",
    "\n",
    "config = {\n",
    "    \"savepath\": 'this.pth',\n",
    "    \"model\": model,\n",
    "    \"classes\": CLASSES,\n",
    "    \"show_classes\": CLASS_NAMES,\n",
    "    \"class_names\": CLASS_NAMES,\n",
    "    \"scheduler\": scheduler,\n",
    "    \"optimizer\": optimizer,\n",
    "    \"npy_dir\": \"good_samples\",\n",
    "    \"timm_model\": \"cnn\",\n",
    "    'learning_rate': learning_rate,\n",
    "    \"num_workers\": 1,\n",
    "    \"pretrain\": 1,\n",
    "    \"epochs\":30,\n",
    "    \"patience\":10,\n",
    "    \"batch_size\":256,\n",
    "    \"seed\":135,\n",
    "    \"loader_seed\":125,\n",
    "    \"num_experts\":4,\n",
    "    \"towers_hidden_dims\":8,\n",
    "    \"towers_outdims\": 4,\n",
    "    \"fusion_hidden_dims\":8,\n",
    "    \"fusion_router_dims\":16,\n",
    "    \"fusion_outdims\":16,\n",
    "    \"weight_exp\": 0.85,\n",
    "    \"max_norm\":1,\n",
    "    \"conv1_channels\": 32,\n",
    "    \"conv2_channels\": 64,\n",
    "    \"conv_kernel\": 5,\n",
    "    \"conv_dropout1\": 0.5,\n",
    "    \"conv_dropout2\": 0.55,\n",
    "    \"meta_fc1_neurons\": 128,\n",
    "    \"meta_fc2_neurons\": 128,\n",
    "    \"meta_dropout\": 0.25,\n",
    "    \"comb_fc_neurons\": 8,\n",
    "    \"comb_dropout\": 0.2\n",
    "}\n",
    "\n",
    "\n",
    "train(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670b3f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TowerBlock(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.25):\n",
    "        super().__init__()\n",
    "        self.metapath = nn.Sequential(\n",
    "            nn.BatchNorm1d(input_dim),\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        path = self.metapath(x)\n",
    "        return path\n",
    "\n",
    "class XastroMiNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Image and Metadata transient classifier\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, config, num_classes=3, num_mlp_experts=4, towers_hidden_dims = 16,\n",
    "                 towers_outdims = 32,\n",
    "                 fusion_hidden_dims = 128,\n",
    "                 fusion_router_dims = 128,\n",
    "                 fusion_outdims = 32\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.towers_hidden_dims = towers_hidden_dims\n",
    "        self.towers_outdims = towers_outdims\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "\n",
    "        self.fusion_hidden_dims = fusion_hidden_dims  # was 1024\n",
    "        self.fusion_router_dims = fusion_router_dims # was 256\n",
    "        self.fusion_outdims = fusion_outdims\n",
    "\n",
    "\n",
    "\n",
    "        # ===== Metadata Processing Towers ===== \n",
    "        \n",
    "        # LC features tower\n",
    "        self.lc_tower = lc_tower()\n",
    "        self.lc_tower.load_state_dict(torch.load('models/lc1_tower.pth'))\n",
    "        # self.lc2_tower = SmallResidualTowerBlock(13, self.towers_hidden_dims*2, towers_outdims*2, do_gating=False, dropout=0.2)\n",
    "\n",
    "        # Spatial features tower (distpsnr1, distpsnr2, nmtchps)\n",
    "        self.spatial_tower = spatial_tower(5, 32, 3)\n",
    "        self.spatial_tower.load_state_dict(torch.load('models/spatial_tower.pth'))\n",
    "\n",
    "        # Nearest source features tower 1 (sgscore1, distpsnr1)\n",
    "        self.nst_tower = nst_tower(2, 16, 2)\n",
    "        self.nst_tower.load_state_dict(torch.load('models/nst1_tower.pth'))\n",
    "\n",
    "        # Coord features tower\n",
    "        self.coord_tower = Coordinate_Tower(2, 128, 1)\n",
    "        self.coord_tower.load_state_dict(torch.load('models/best_coord_tower.pth'), strict=False)\n",
    "\n",
    "        self.mega_tower = TowerBlock(24, 128, 128)\n",
    "\n",
    "\n",
    "\n",
    "        # ===== Image Processing =====\n",
    "\n",
    "        self.conv_branch = nn.Sequential(\n",
    "                nn.Conv2d(3, config['conv1_channels'], \n",
    "                        kernel_size=config['conv_kernel'], padding='same'),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(config['conv1_channels'], config['conv1_channels'], \n",
    "                        kernel_size=config['conv_kernel'], padding='same'),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Dropout(config['conv_dropout1']),\n",
    "                \n",
    "                nn.Conv2d(config['conv1_channels'], config['conv2_channels'], \n",
    "                        kernel_size=config['conv_kernel'], padding='same'),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(config['conv2_channels'], config['conv2_channels'], \n",
    "                        kernel_size=config['conv_kernel'], padding='same'),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(4),\n",
    "                nn.Dropout(config['conv_dropout2']),\n",
    "                \n",
    "                nn.Flatten()\n",
    "            )\n",
    "        \n",
    "        self.conv_output_size = 2304\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # fusion_dims = 6*towers_outdims + 2*fusion_outdims + 1 \n",
    "        fusion_dims = self.conv_output_size + 128 + 1 + 3 + 3 + 2 + 1\n",
    "        # ===== Modality Fusion MoE ===== \n",
    "        # Combines features from all towers (8 metadata + image)\n",
    "        # self.fusion_experts = nn.ModuleList([\n",
    "        #     ResidualExpertBlock(fusion_dims, fusion_hidden_dims, num_classes, do_gating=False, dropout=0.5)\n",
    "        #     for _ in range(num_mlp_experts)\n",
    "        # ])\n",
    "\n",
    "        self.num_mlp_experts=num_mlp_experts\n",
    "        num_experts=num_mlp_experts \n",
    "\n",
    "\n",
    "        self.fusion_tower = nn.Sequential(\n",
    "            nn.Linear(128 + 1 + 3 + 3 + 2 + 1, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(8, num_classes),\n",
    "            # nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # self.bts_bot = nn.Sequential(\n",
    "        #     nn.Linear(fusion_dims, 8),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Dropout(0.2),\n",
    "        #     nn.Linear(8, num_classes),\n",
    "        #     # nn.Sigmoid()\n",
    "        # )\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        self.fusion_router = nn.Sequential(\n",
    "            nn.Linear( fusion_dims, fusion_dims//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fusion_dims//2, num_experts),\n",
    "\n",
    "        )\n",
    "        \n",
    "\n",
    "\n",
    "            \n",
    "    def forward(self, metadata, image=None, training=True):\n",
    "\n",
    "        # Process all metadata features through respective towers\n",
    "        lc_feats = self.lc_tower(metadata[:, [6, 9, 10, 13, 15, 17, 18, 19, 20, 21, 22, 23]])\n",
    "\n",
    "        spatial_feats = self.spatial_tower(metadata[:, [0,1,2,3,4]])  # Spatial features\n",
    "\n",
    "        nst = self.nst1_tower(metadata[:, [0,2]])  # Nearest source A features\n",
    "\n",
    "        coord_feats = self.coord_tower(metadata[:, [7,8]])\n",
    "        \n",
    "        \n",
    "        # # Process image if available (zeros otherwise)\n",
    "        # image_feats = self.image_tower(image) \n",
    "        # image_feats = nn.Dropout(0.4)(image_feats)\n",
    "\n",
    "\n",
    "\n",
    "        # Concatenate all features for fusion\n",
    "        all_other_feats = torch.cat([nst, spatial_feats, coord_feats, lc_feats ], dim=1)\n",
    "        # all_other_feats = nn.Dropout(0.3)(all_other_feats)\n",
    "\n",
    "\n",
    "        mega_in_feats = torch.cat([ metadata[:, [0,1,2,3,4,5,6,7,8,9,10,11,12, 13, 14,15, 16, 17, 18, 19, 20, 21, 22, 23]]], dim=1)\n",
    "        megatower = self.mega_tower(mega_in_feats)\n",
    "\n",
    "        fused_feats = self.fusion_tower(torch.cat([all_other_feats, megatower], dim=1))\n",
    "\n",
    "\n",
    "        # all_feats = torch.cat([all_other_feats, megatower, image_feats], dim=1)\n",
    "        # bts_feats = self.bts_bot(all_feats)\n",
    "\n",
    "\n",
    "        # all_feats = torch.cat([megatower, image_feats], dim=1)\n",
    "        # all_feats = nn.Dropout(0.4)(all_feats)\n",
    "        \n",
    "        # # Fusion MoE - combine features from all modalities\n",
    "        # # Fusion MoE - combine features from all modalities\n",
    "        # fusion_logits = self.fusion_router(all_feats)\n",
    "        # fusion_weights = nn.Softmax(dim=-1)(fusion_logits)\n",
    "\n",
    "        # # Get top-k experts for each sample\n",
    "        # k = min(2, self.num_mlp_experts)  # k=2 if more than 1 expert, else 1\n",
    "        # topk_weights, topk_indices = torch.topk(fusion_weights, k=k, dim=-1)  # [B, k]\n",
    "\n",
    "        # # Initialize output\n",
    "        # moe_output = torch.zeros(metadata.size(0), self.num_classes, device=metadata.device)\n",
    "\n",
    "        # # Process through each expert\n",
    "        # for expert_idx, expert in enumerate(self.fusion_experts):\n",
    "        #     # Find samples that use this expert in any of their top-k positions\n",
    "        #     expert_mask = (topk_indices == expert_idx).any(dim=1)  # [B]\n",
    "            \n",
    "        #     if not expert_mask.any():\n",
    "        #         continue\n",
    "            \n",
    "        #     # Get weights for this expert across all top-k positions\n",
    "        #     weights = torch.zeros_like(expert_mask, dtype=torch.float32)  # [B]\n",
    "        #     for k_pos in range(topk_indices.size(1)):\n",
    "        #         k_mask = (topk_indices[:, k_pos] == expert_idx)\n",
    "        #         weights[k_mask] += topk_weights[k_mask, k_pos]\n",
    "            \n",
    "        #     # Compute expert output only for relevant samples\n",
    "        #     expert_out = expert(all_feats[expert_mask])  # [M, num_classes]\n",
    "            \n",
    "        #     # Weighted contribution\n",
    "        #     moe_output[expert_mask] += weights[expert_mask].unsqueeze(-1) * expert_out\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        return {\n",
    "            'logits': fused_feats,\n",
    "            'expert_weights': fused_feats,\n",
    "            'fusion_weights': fused_feats\n",
    "        }\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3035734e",
   "metadata": {},
   "source": [
    "### 2b:\n",
    "#### go back now and make the missing towers here!!\n",
    "\n",
    "train each missing tower on a class grouping you see most fit, then set the big model up to preload the saved best towers, and train for more classes!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbaf24ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16da60a2",
   "metadata": {},
   "source": [
    "### 2c:\n",
    "#### go back now and add the cnn tower!!\n",
    "\n",
    "does it improve things?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ztfss25",
   "language": "python",
   "name": "ztfss25"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
